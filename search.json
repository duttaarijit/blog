[
	
		{
		  "title"    : "To the Goth Kids",
		  "category" : "career",
		  "url"     : "/career/to-the-goth-kids/",
		  "date"     : "2017-07-28 00:00:00 +0530",
		  "content"	: "<br /><br />It seems like only recently , startups would hire folks to do just about everything , all at once , and then somehow , we’ve arrived at a ‘Product Manager of Paytm Experience’. How tunnel vision syndrome is eroding the startup spirit.<br /><br /><br />Recently , a startup valued at around half a billion dollars laid off 10 of its Product Managers , leaving them with another 30. Just last year , the same startup hired Product guys at 20–30% premium from the market at average CTCs well over ₹20L. Even more intriguing , some of these guys were techie turned MBAs with less than four years’ experience. Having always been on the sales side of things , where one had to justify a minimum of 4x one’s salary to the company , I was fascinated by this lot. What did they do that was worth that much money?<br /><br />One product guy I spoke to said he managed Paytm experience; which meant he had to ensure there were no drop-offs when the user chose to pay through Paytm. He also said he was mandated to prioritise payment through Paytm. And there were similar folks for each of the other alternate payment processes. “But , is Paytm the most competitive payment solution?” , I asked. He didn’t care , really. All that mattered was ‘mukesh23’ got through paying on the platform without , gods forbid , choosing to click on the refresh button. Even better if he came through one of the exclusive Paytm promotions that he had brokered with his counterpart from the other side.<br /><br />Evidently , the metric for success — # completed transactions , is not entirely off-base. A good product guy could save the company millions , potentially. But , on closer observation it seems as though his chutzpah might also cost the company millions , if not more. In the above case , there are several problems with how the roles are structured. What if Paytm wasn’t the best payment option on the platform? What if (plain conjecture , here) it were costlier , for instance? What if these users exited the native platform at rates higher than average?<br /><br />Let’s leave those seemingly troubling questions aside for a moment. What does one do to improve a third party payment experience? Mainly , vary size and placement of the button , apparently. Turns out , there isn’t much you can do. But , that doesn’t mean you can’t do nothing. So , if you notice a needless “improvement” in your experience , know that some Product Manager’s review is forthcoming. Then , is it a surprise , really , that when things take a turn for the worse , these lot are first in the line of fire? I was surprised , though , to understand that a lot of them were entirely at peace with the transitional nature of their employment. 30% elsewhere , then.<br /><br />This is not restricted to Product folks alone , although it does seem like in recent years it has become a “get rich scheme” of sorts for some techies with an acute propensity to bullshit their way through things. I see Marketing Managers who can’t / won’t write a line of copy or tweak keywords on their website. I see Designers who can’t / won’t code simple HTML or work on user personas and flow maps. I see Engineers who can’t / won’t test their code or learn how to write coherent software requirement specifications. This is manifestly due to the over-specialisation of roles and warped organisation structures in these startups. Ergo , the bleeding disinterest.<br /><br />It wasn’t always like this. It used to be that startups hired people to do just about everything , all at once.<br /><br />Fresh out of college , I was hired as a ‘Management Trainee’ , which I came to realise was code for will do whatever the hell it takes to move the needle. In the first year alone I did Sales , Product , Operations , and Marketing. I wasn’t alone; it seemed like everybody did everything. I remember our VP-Technology managing client delivery for a new initiative , and doing a damn good job at that. I remember our Operations Manager writing bizarre VBA Macros for Excel that saved hours of effort. And it seemed everybody everywhere else , too , were running an arm and a leg short of the work that was on their plates. It was synonymous with startups.<br /><br />It made tremendous business sense , too. First , it was easier to find (and afford) people at the median levels of overlapping skill sets than at the top 1% of their specialisations. Second , people always had a macro view of the company’s goals and everybody , more or less , aligned their personal work accordingly. Third , it had unexpected , yet , massive pay-offs for our chosen specialisations: techies wrote better code because they understood business and sales guys were more effective because they knew the real implications of that code. It wasn’t easy , but those years probably had the greatest impact in my life. It schooled my thoughts and perspectives.<br /><br />When people ask me what changed over the years , I give them the following analogy: a startup , back in the day , was like a goth band. It attracted the misfits , the weirdos , and those of us who just happened to stumble into the mosh pit. There was a ton of work to do and very little real money to be made. You belonged to somewhat of a cult and expected unreasonable things of yourself and your brethren. What we lacked in resources we made up for with ingenuity and perseverance. Over the years , however , the goth band got a makeover. We couldn’t have been more thrilled at that time. It seemed like the World was finally giving us our due. We didn’t have to explain to mothers , uncles and landlords , what we did for a living. We could finally afford EMIs.<br /><br />But , gradually , the goth kids turned cool. The makeup , now , seemed superficial and the cult constantly disowned its own — for how much wisdom can be gained from tweaking button sizes over years? The law of diminishing marginal returns applied: increasing number of new members to the cult caused the marginal product of others to be smaller than the marginal product of the previous members at this point. And that’s how we got ‘Product Manager of Paytm Experience’.<br /><br />But , what of the goth kids? They do lurk around. You won’t find them in conferences or hackathons or by the vending machines mooching off’ the free stuff. They’re likely in an intimate corner , head immersed in the laptop , being productive and maybe checking on Twitter once a while. If you’re ever in the position of hiring for your startup , my suggestion is for you to find and hire the goth kid. He’ll remind you why you started up in the first place. And also , he won’t ask you about your company’s pet policy.<br /><br /><br /><br />  This article was originally published  on Medium. We are republishing it here with the permission of the author<br /><br /><br /><br />"
		} ,
	
		{
		  "title"    : "Setting up OAuth2 callbacks in Rails with HTTPS offloading on load balancers",
		  "category" : "technology",
		  "url"     : "/technology/setting-up-elb-plus-nginx-with-https-offloading/",
		  "date"     : "2017-03-08 00:00:00 +0530",
		  "content"	: "“HTTPS everywhere” is not a luxury anymore. It is a necessity. Thankfully , obtaining an SSL certificate has become easier too , with initiatives such as Let’s Encrypt , GeoTrust , Positive SSL , StartSSL. Even cloud based services such as Cloudflare and Amazon AWS provide free SSL certificates to their customers.<br /><br />####Here is setting some context to help the reader appreciate the discussion:<br />We host our rails applications on Amazon AWS. We generally use three different environments - development , staging and production. Development environment is generally local to a developer while staging and production are hosted on the cloud. There is a minor difference in the way we configure our staging and production environments. Our staging environment typically contains a single machine instance hosting our application. This single instance is exposed to internet directly (has a public IP). On the other hand , our production environment typically contains a cluster of instances for the sake of horizontal scaling. These instances typically do not have a public IP and hence not exposed to internet directly. We put this cluster behind an internet-facing Elastic Load Balancer (ELB).<br /><br />We use chef-solo to manage our cloud infrastructure as well as to deploy code to various environments.<br /><br />#####The Problem Statement:<br />For the sake of this discussion , we shall limit ourselves to configuring SSL certificates obtained from the two free providers , namely Let’s Encrypt and Amazon AWS.<br /><br />Using Let’s Encrypt in a clustered setup is tricky , since you need to make one of the instances stateful , in the sense , one instance needs to be given the responsibility of obtaining and renewing SSL certificate from Let’s Encrypt. All other instances need to copy this certificate every time its renewed. This requirement unnecessarily complicates the setup and also takes away some amount of flexibility. Also , Let’s Encrypt does not issue wildcard certificates and the validity of a certificate is just 90 days<br /><br />The certificates provisioned from the other provider , Amazon AWS , can only be installed on an ELB. Hence is best suited for our clustered setup , namely production. An added advantage is that Amazon can issue wildcard certificates. We could always add an ELB to our staging environment (even though we will never have more than one instance) , but that costs extra money for no reason.<br /><br />This leaves us with these options<br /><br /><br />  <br />    Environment<br />    Best Option<br />  <br />  <br />    Staging<br />    Let's Encrypt<br />  <br />  <br />    Production<br />    Amazon AWS<br />  <br /><br /><br />We went ahead with this choice. Using chef to manage our setup came handy.<br /><br />We first configured our Staging environment and everything worked as expected.<br /><br />However , the same application , in production environment , started throwing CSRF detected Error whenever an OAuth2 callback happened. This was really strange. Our application integrated with two different OAuth providers , and the problem was consistent with both these providers.<br /><br />#####What’s the issue?<br /><br />The only difference between our Staging and Production setups was the ELB.<br /><br />In production , we offloaded HTTPS at the ELB. Plain HTTP request would hit the NGINX web server , which in turn would reverse-proxy it to unicorn and rails.<br /><br />CSRF detected was clearly an error emitting from the rails application. Not from NGINX , and not from the ELB.<br /><br />A closer look would reveal that the rails application had no way to know if the callback was made on a http:// URL or a https:// URL , because it sees only HTTP (due to offloading).  Was this the reason rails was unhappy?<br /><br />OAuth2 , by design , does not accept plain HTTP callbacks (unless it is to localhost).<br /><br />####How do we move forward?<br /><br />#####PoC to prove the theory<br /><br />Just to confirm what we think is the cause , we enabled HTTPS on NGINX (like we did in our staging environment). This was in addition to HTTPS on the Load balancer. We reconfigured the Load Balancer to NOT offload HTTPS but forward the request as-is to NGINX.<br /><br />What do we have now? The CSRF detected errors are gone. Application behaves just like it should.<br /><br />This confirmed our theory.<br /><br />But the question now is , how do we achieve our desired configuration of offloading HTTPS at the ELB ? Is it just not possible ?<br /><br />The Solution<br /><br />We have been using X-Forwarded-For header while reverse proxying to unicorn so that our rails application knows the client IP address (rather than the IP address of the Load Balancer). We need this for logging and tracking.<br /><br />Could there be something on similar lines to tell the rails application that the request was not on HTTP but on HTTPS?<br /><br />Sure there is. We had to set a header in our reverse proxy configuration:<br /><br />X-Forwarded-Proto  to  https<br /><br /><br />For NGINX , we do it like this:<br /><br />  proxy_set_header X-Forwarded-Proto https;<br /><br />Voila , Rails is happy and things are back to normal!<br /><br />Details:<br /><br />Csrf detected!<br /><br />Rails bothers about SSL only at two places , <br /><br /><br />  At environment config , force_ssl.<br /><br />  At external included Gem like Omniauth. <br /><br /><br /><br />In Rails environment config.<br /><br />  config.force_ssl = true<br /><br />This does the trick , but doesn’t seem like a good idea to enable this option in Rails because , we offload https at NGINX. For Rails , request came in http , so it does a permanent redirect to https , which ends in a infinite loop.<br /><br />Our stack trace gave a clue that error might be inside omniauth gem.<br /><br /><br />    actionpack-4.2.7.1/lib/abstract_controller/base.rb:132 → process<br />    actionview-4.2.7.1/lib/action_view/rendering.rb:30 → process<br />    actionpack-4.2.7.1/lib/action_controller/metal.rb:196 → dispatch<br />    actionpack-4.2.7.1/lib/action_controller/metal/rack_delegation.rb:13 → dispatch<br />    actionpack-4.2.7.1/lib/action_controller/metal.rb:237 → block in action<br />    actionpack-4.2.7.1/lib/action_dispatch/routing/route_set.rb:74 → dispatch<br />    actionpack-4.2.7.1/lib/action_dispatch/routing/route_set.rb:43 → serve<br />    actionpack-4.2.7.1/lib/action_dispatch/journey/router.rb:43 → block in serve<br />    actionpack-4.2.7.1/lib/action_dispatch/journey/router.rb:30 → each<br />    actionpack-4.2.7.1/lib/action_dispatch/journey/router.rb:30 → serve<br />    actionpack-4.2.7.1/lib/action_dispatch/routing/route_set.rb:817 → call<br />    omniauth-1.3.1/lib/omniauth/strategy.rb:186 → call!<br />    omniauth-1.3.1/lib/omniauth/strategy.rb:164 → call<br /><br /><br />As we dug inside the Gem and found out that Omniauth looks at these headers<br /><br />lib/omniauth/strategy.rb#L493-L499<br /><br />def ssl?<br />  request.env[&amp;#39;HTTPS&amp;#39;] == &amp;#39;on&amp;#39; ||<br />  request.env[&amp;#39;HTTP_X_FORWARDED_SSL&amp;#39;] == &amp;#39;on&amp;#39; ||<br />  request.env[&amp;#39;HTTP_X_FORWARDED_SCHEME&amp;#39;] == &amp;#39;https&amp;#39; ||<br />  (request.env[&amp;#39;HTTP_X_FORWARDED_PROTO&amp;#39;] &amp;amp;&amp;amp; request.env[&amp;#39;HTTP_X_FORWARDED_PROTO&amp;#39;].split(&amp;#39; ,&amp;#39;)[0] == &amp;#39;https&amp;#39;) ||<br />  request.env[&amp;#39;rack.url_scheme&amp;#39;] == &amp;#39;https&amp;#39;<br />end<br /><br />This is where we found that setting up X_FORWARDED_PROTO to https should fix our problems.<br /><br />Initially , this X_FORWARDED_PROTO was set to $scheme. Which will be http for production as https is offloaded at ELB.<br /><br />Now , by setting X_FORWARDED_PROTO to https , we are making sure that redirects are happening on https.<br /><br />"
		} ,
	
		{
		  "title"    : "Custom Capacity Buffers In Go",
		  "category" : "technology",
		  "url"     : "/technology/custom-capacity-buffers-in-go/",
		  "date"     : "2015-03-31 00:00:00 +0530",
		  "content"	: "At elitmus we use ruby to create most of our tools and most of our applications  are also written in ruby. Recently I started exploring ways to build our tools especially the backend tools in languages other than ruby which have much lesser memory footprint and better efficiency. One such cases was to create a sandboxed environment for running  untrusted code on our servers. After evaluating multiple languages , I decided to use golang because of it’s excellent library support coupled with the fact the docker(a sandboxed env) was also written in go.<br /><br />One of the many challenges we faced while creating our sandbox was  redirection of the standard output of untrusted code , as this simple code below will fill up the disk if redirected to file or use all system resources if redirected to a buffer.<br /><br />1 while(true) printf(“I am the green monster”);<br /><br />So the problem is ,how to limit the size of a file or buffer? , I  started with   buffers  as they are more easy to implement.  I assumed that the Write method of the Buffer struct which writes to the buffer , will panic with ErrTooLarge error if buffer size is above it’s capacity , which i hoped to catch using recover builtin function.<br /><br />This is the code snippet below.<br /><br /> 1    defer func() {<br /> 2       if r := recover(); r != nil {<br /> 3          fmt.Println(&ldquo;Should catch if anyone panics&ldquo;)<br /> 4       }<br /> 5    }()<br /> 6   a := bytes.NewBuffer(make([]byte , 0 , 2))<br /> 7   for {<br /> 8     _ ,err := a.Write([]byte(&ldquo;create boom&ldquo;))<br /> 9     if err != nil {<br />10       fmt.Println(err.Error())<br />11        return<br />12     }<br />13 <br />14   }<br /><br />On running this code , my system was frozen and crashed a little later. This is not what i expected , On further investigation by looking to source code and reading the bytes package documentation again , i found out that Write method in the bytes package is growing the capacity of the  buffer if the buffer capacity is not enough , which in turn is increasing the amount of memory and resources used by the system.<br /><br />After some googling and with good help from the go community(thanks to dave cheney) , i decided  to create wrapper around the buffer struct and implement my own io.Writer interface by implementing Write method for the wrapper which writes to the buffer.<br /><br />My custom wrapper’s will take capacity as parameter when initializing and the Write method will do the required action if there is a buffer overflow , instead of increasing the capacity like the Write method from bytes package. This is done by monitoring the size of the buffer before writing to the buffer.<br /><br />This is code snippet of my custom wrapper.<br /><br /> 1 type MyBuffer struct {<br /> 2     cap   int<br /> 3     mybuf *bytes.Buffer<br /> 4 }<br /> 5 <br /> 6 func (b *MyBuffer) Write(p []byte) (n int , err error) {<br /> 7     if len(p)+b.mybuf.Len() &#62;; b.cap {<br /> 8         fmt.Printf(b.mybuf.String())<br /> 9         panic(&ldquo;Buffer Overflow&ldquo;)<br />10     } else {<br />11         b.mybuf.Write(p)<br />12     }<br />13     return len(p) , nil<br />14 }<br />15 <br />16 func NewBuffer(buf []byte , cap int) *MyBuffer {<br />17     return &amp;amp;MyBuffer{mybuf: bytes.NewBuffer(buf) , cap: cap}<br />18 }<br />19 <br />20 func main() {<br />21 <br />22     defer func() {<br />23         if r := recover(); r != nil {<br />24             fmt.Println(&ldquo;recover in yes&ldquo;)<br />25         }<br />26     }()<br />27 <br />28     a := NewBuffer(make([]byte , 0 , 100) , 200)<br />29     for {<br />30         _ , err := a.Write([]byte(&ldquo;Check for Buffer Overflow&ldquo;))<br />31         if err != nil {<br />32             fmt.Println(err.Error())<br />33             return<br />34         }<br />35     }<br />36 }<br /><br />On running this code , it worked as expected , hopefully will be deployed in production.<br />The same goes for files as well.<br /><br />Note: useful links , on docker ,on golang bytes package<br /><br />"
		} ,
	
		{
		  "title"    : "Making Airtel 3G dongle work on Mac OS 10.10 Yosemite",
		  "category" : "technology",
		  "url"     : "/technology/making-airtel-3g-dongle-work-on-mac-os-10-dot-10-yosemite/",
		  "date"     : "2014-12-03 00:00:00 +0530",
		  "content"	: "If you use Airtel 3G Dongle (Mine is Huawei E173) on your Mac , and are having issue using the dongel after upgrading to Yosemite , airtel is of little help. They asked me to downgrade the OS to Mavericks!<br /><br />The reason why the dialer software provided by airtel does not work is , that they internally use Apple USB Modem. According to this FAQ on apple support site , your Operating system should be running in 32 bit mode for the modem to work. Yosemite however , is 64 bit.<br /><br />Anyway , I could find multiple ways to overcome the problem. Here I am writing about the most simple one<br /><br />###Step 1: <br />Click on this link to download the new compitable driver from Huawei website Mac-V200R003B015D11SP00C983(for Mac10.10).rar<br /><br /><br /><br />###Step 2: <br />Open the archive , you will find two files<br /><br />1. Mobile Partner install user guide.docx<br />2. Mobile Partner.zip<br /><br /><br />The word document has detailed instructions with screenshots , on how to install.<br /><br />###Step 3: <br />Open the zip file Mobile Partner.zip , you will find Mobile Partner.app. Double click on this file to install the app<br /><br />###Step 4: <br />Once installed , start the app and go to Tools -&#62;; Options<br /><br /><br /><br />###Step 5: <br />In the Options window , choose “Profile Management” from the left side menu<br /><br /><br /><br />###Step 6: <br />Click on “New” button to create a new profile. Give it a name , such as “airtel 3g”. Also , make sure the “Access Number” is set to *99#. Click “Save” , then “Ok”.<br /><br /><br /><br />###Step 7: <br />Insert your Dongel into an USB port. You should see “Mobile Partner” application starting automatically. Choose the profile you created in Step 6 (“airtel 3g”) and “Connect”.<br /><br /><br /><br />That’s it.<br /><br />"
		} ,
	
		{
		  "title"    : "IT Career - Pitfalls to avoid",
		  "category" : "career",
		  "url"     : "/career/it-career-pitfalls-to-avoid/",
		  "date"     : "2014-09-22 00:00:00 +0530",
		  "content"	: "It is very humbling when a youngster walks up to us and says “Thanks for helping me get my first job”. While we are delighted at one end , we are worried at the other.  Why? Because , most of the time a fledgling mind does not see the disaster ahead! Yes , we mean disaster – 75% of IT professionals of 2011-2014 batches will be unemployed 20 years from now.  And this is assuming IT industry does well !! Looks unlikely? Read on to know more.<br /><br />Indian IT industry has employed around 7 ,50 ,000 professionals from the four batches (2011 , 2012 , 2013 , 2014). An estimated 1 ,50 ,000 of these will leave the Indian IT industry to pursue higher studies and never come back to work for the same industry. That leaves us with 6 ,00 ,000 professionals who will be in the industry for long. The question is: How long? Being highly paid with around 20 years of experience , in the year 2030 , companies would want them to take larger responsibilities and oversee at least 100 professionals under them. Summing it up , these 6 lakh professionals should have 6 crore professionals below them. Assuming IT industry grows at 10% per annum for 20 years (caution - it may already be slowing down) , the whole industry will be just 1.2 crore strong. That means at most 1.2 lakh senior professionals will be needed. What would happen to the rest 4.8 lakh professionals? They would , of course , be unemployed.<br /><br />Difficult to digest? In 1995 , there were approximately 11 ,000 software professionals across all levels. Nearly 50% of them are now citizens of another country or earned enough money from the exponentially growing market (nascent market then) growing market to retire , appropriately called VIP (vested in peace). Another at most 15 ,500 professionals of the same era migrated into IT industry from other industries (like SAP consultants , Supply Chain , Financial professionals). So that is a conservative 21 ,000 senior professionals in the whole of Indian IT industry. Many of these who lose a job today struggle to find another suitable profile (20 Yrs of experience) and this is when growth rates in this period have been over 25%. You can see it happening for the current 40+ year old professionals!!<br /><br />Now you have a lingering doubt – could there be something wrong in the projections? Yes !! But it is unfortunately on the negative side. What if the industry grows slower than 10% (may be another bad patch of no growth for 2-4 years). What if automation makes many more jobs redundant (now in IT itself , think about it!)? Last but not the least , another country taking away jobs from India (like China did in manufacturing)?<br /><br />A tell-a-tale from not very long ago is the textile mills of Bombay. They were teeming with activity and nothing could go wrong for them in 1970s and early 1980s. It could only get better as population was growing and people’s ability to spend was increasing. These very mills today are malls!! It can be argued to be a ‘crowding out’ phenomenon , surely not applicable to sunrise IT industry. Or maybe it is visible only in hindsight !!!<br /><br />What are we doing at eLitmus to help the cause?<br /><br /><br />  <br />    We are pushing companies not to lower the entry barriers. We have found that immediately after a slow down year , quality and quantity of candidates improve. Quantity ok , but how quality? You call it competition , you call it lowered demand or call it buyers market. So if companies can adopt the quality principals in this period , why not in growth phase as well. It will help students also.<br />We strongly believe a youngster`s ability to adapt and evolve is much higher than an older person. So push them today rather than tomorrow. If you remember your grandparents had the fitness and ability to walk kilometres at their old age (not spoilt by automobiles when they were a child)<br />  <br />  <br />    Educating students that the easy path out , though rosy for short term , will destroy them. We want them to go that extra mile. That explains our rigorous question paper which tests fundamentals and concepts. We want them to earn their job rather than get it. In the process their ability goes up. Few students who wrote pH test in the initial years of eLitmus have founded their own start-ups.<br />  <br />  <br />    Ensuring start-ups and companies with great work environment do not struggle for lack of talent. Most of these engagements are loss making. We survive thanks to the fact that most of our colleagues at eLitmus are passionate about what they do and work at a fraction of their market salary!!<br />  <br /><br /><br />We are committed to “making India competitive” and we hope we have challenged the young reader of this blog to go the extra mile. As Steve Jobs once quoted the Whole Earth catalogue “Stay hungry , Stay foolish!”<br /><br />"
		} ,
	
		{
		  "title"    : "How we host our blog on GitHub pages and yet serve it from our own Sub-URL",
		  "category" : "technology",
		  "url"     : "/technology/how-we-host-our-blog-on-github-pages-and-yet-serve-it-from-our-own-sub-url/",
		  "date"     : "2014-08-18 00:00:00 +0530",
		  "content"	: "There are umpteen number of blog posts telling you how to host your static site on GitHub Pages for free. They also tell you how to serve such a site from your own domain name.<br /><br />As you may have guessed , this blog is also hosted on GH Pages. Don’t believe me? try visiting this URL https://shireeshj.github.io/blog/<br /><br />It is easy to map a github.io url such as this , to a subdomain. For example , it is easy to map the url to https://blog.elitmus.com/blog/.  All you need to do is check-in a file named CNAME into the root folder of your git repo that contains your static site.<br /><br />What if you want your static site to be served from domain apex? That is easy too.  GitHub pages help explains this in a simple manner.<br /><br />However , If your domain apex is already taken , say by your other website , you have a problem.  To host your static site on a domain apex (or a sub-url of domain apex) the domain apex should be available exclusively for use by github pages.<br /><br />We had to overcome this very problem , since our business website is already hosted on elitmus.com (and www.elitmus.com).  Given that we are not in great love with subdomains. We had to find a workaround. And here is what we did:<br /><br />Since we use nginx to server our business website , all we had to do was to write a simple traffic-cop rule. What this rule did was , to parse the request url to see if it starts with /blog/. If yes , then the request is reverse proxied to GitHub Pages. If no , then it is served from local disk.<br /><br />The relevant lines from the config file are here<br /><br /><br/>location /blog/ {<br /><br/>    proxy_pass       http://shireeshj.github.io/;<br /><br/>    proxy_redirect off;<br /><br/>    proxy_set_header Host &#60;;shireeshj.github.io&#62;;;<br /><br/>    proxy_set_header X-Host &#60;;shireeshj.github.io&#62;;;;<br /><br/>    proxy_set_header X-Real-IP $remote_addr;<br /><br/>    proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;<br /><br/>  }<br /><br /><br />This is how we get free hosting for our blog , yet serve it from our official URL. If GitHub pages ever stops us from reverse proxying , we shall simply spin our own webserver to run this static site and reverse proxy to that web server.<br /><br />"
		} ,
	
		{
		  "title"    : "Using Monit to get email alert on unauthorized login",
		  "category" : "technology",
		  "url"     : "/technology/using-monit-to-get-email-alert-on-unauthorized-login/",
		  "date"     : "2014-06-04 00:00:00 +0530",
		  "content"	: "For a long time , we had our own custom written perl script to alert us whenever someone logged into our production servers from an ip address we do not recognize (not whitelisted). The script looked somewhat like this…<br /><br />#!/usr/bin/perl<br /># script file: alert_on_login.pl<br />#<br />my $login_str = &ldquo;Accepted publickey&ldquo;<br />my $whitelist_ip = &ldquo;122.123.123.111&ldquo;<br /><br />sub sendEmail<br />{<br />        my ($to , $from , $subject , $message) = @_;<br />        my $sendmail = &amp;#39;/usr/lib/sendmail&amp;#39;;<br />        open(MAIL , &ldquo;|$sendmail -oi -t&ldquo;);<br />        print MAIL &ldquo;From: $from&ldquo;<br />        print MAIL &ldquo;To: $to&ldquo;<br />        print MAIL &ldquo;Subject: $subject&ldquo;<br />        print MAIL &ldquo;$message&ldquo;<br />        close(MAIL);<br />}<br /><br />while (&#60;;&#62;;) {<br />        if (grep(/$login_str/ , $_) &amp;amp;&amp;amp; !grep(/$whitelist_ip/ , $_)) {<br />                print $_;<br />                chomp $_;<br />                @arr = split(&amp;#39; &amp;#39; , $_);<br />                sendEmail(&amp;#39;recepient1@elitmus.com , recepient2@elitmus.com&amp;#39; ,<br />                          &amp;#39;monit@elitmus.com&amp;#39; ,<br />                          &amp;#39;Server login from &amp;#39; . $arr[10] ,<br />                          $_);<br />        }<br />}<br /><br />All we needed to do was to run this script in the background as a daemon , and it would send us an email alert whenever someone logged in successfully. As root user start the script like this:<br /><br />  # (perl alert_on_login.pl /var/log/auth.log &amp;amp;)<br /><br /><br />Ever since we started using monit for the usual purpose (monitoring processes) , we have also entrusted monit to do the job of the above perl script. Monit makes this super simple…<br /><br />Monit is a popular opensource process monitoring tool. It is used mostly for monitoring health of any linux process and take necessary action if any of the set parameters are breached. Monit can restart a process if the process failed for some reason. Monit can also notify you of incidents and actions taken.<br /><br />See this to learn more about monit’s alert capabilities.<br /><br />Monit’s global configuration file is usually /etc/monit/monitrc. Here is what monit needs to be told about how to send email alerts:<br /><br />...<br /># This is our SMTP server settings. The complete syntax is<br /># SET MAILSERVER &#60;;hostname [PORT] [USERNAME] [PASSWORD] [using SSLAUTO|SSLV2|SSLV3|TLSV11|TLSV12] [CERTMD5 checksum]&#62;; , ...<br />#          [with TIMEOUT X SECONDS]<br />#          [using HOSTNAME hostname]<br />#<br /># But for our purpose , localhost is good enough<br />SET mailserver localhost<br /><br /># This is the email template for alert messages<br />SET mail-format {<br />  from: monit@elitmus.com<br />  subject: $SERVICE $EVENT at $DATE<br />  message: Monit $ACTION $SERVICE at $DATE on $HOST: $DESCRIPTION.<br />           Yours sincerely ,<br />           monit<br />}<br /><br /># Alerts can be triggered for various reasons. Successful ssh login is just one of those reasons.<br /># Since this is a global configuration , we can tell monit to not send alerts for certain events<br />#  We also specify the email address of the recepient who will receive the alerts<br /><br />set alert recepient1@elitmus.com NOT ON { action , instance , pid , ppid , nonexist }<br />...<br /><br />And then we add this config file ssh_logins.conf specific to sshd related stuff:<br /><br />check file ssh_logins with path /var/log/auth.log<br />  ignore match &ldquo;/etc/monit/whitelist_ips.regex&ldquo;<br />  if match &ldquo;Accepted publickey&ldquo; then alert<br /><br />Notice how we tell monit to ignore logins from known ip addresses. We can now store all whitelist ip addresses in a separate file /etc/monit/whitelist_ips.regex , one address per line.<br /><br />Note: We have disabled password based login and hence do not monitor for passworded logins. If you use passworded login , you should change &quot;Accepted publickey&quot; to &quot;Accepted password&quot;<br /><br />Happy monitoring!<br /><br />"
		} ,
	
		{
		  "title"    : "Gotcha&#39;s while syntactically translating AES encryption logic from PHP to Ruby",
		  "category" : "technology",
		  "url"     : "/technology/gotchas-while-syntactically-translating-aes-encryption-logic-from-php-to-ruby/",
		  "date"     : "2014-05-25 00:00:00 +0530",
		  "content"	: "Our Payment Gateway service provider recently launched a new platform with some nice-to-have features. We wanted those features and so we decided to migrate. Being one of the earliest adopters of the new platform , there was no integration kit available. We had to build it ourselves. Not a problem. Since we are a Ruby On Rails shop , we built our own Ruby integration kit. All went well and we pushed it to production.<br /><br />A month or two later , we got an email from our gateway provider seeking our help with writing the encryption and decryption logic for the Ruby integration kit they were developing. We were a little surprised , because we noticed they had already published integration kits for PHP , Python , JAVA etc. How difficult can it be to translate that to Ruby?<br /><br />Turns out , syntactic transalation of code from one programming language to another does not always work. A slightly more deeper knowledge helps. We could almost guess where they were getting stuck.<br /><br />Before we get to the story , some backgroung on the encryption algo will add clarity.<br /><br />For secure communication between our server and the gateway , the prescribed cipher was AES , specifically symmetric-key block cipher with a 128 bit secret key in CBC mode. Since OpenSSL already implements this algo and is avaliable on almost all platforms , most programming languages just bundle a wrapper for OpenSSL.<br /><br />So if its the same OpenSSL that the wrappers call , why couldn’t the gateway service provider translate their own PHP code to Ruby?<br /><br />Here is why:<br /><br />AES works by breaking the plain text (the text to be encrypted) into blocks of 128 bits (or 16 bytes). In CBC mode , each block is XORed with the key to get cipher text of that block. The cipher text of the previous block is used for encrypting the next block… so on and so forth , until all the blocks are encrypted.<br /><br />Note that the length of the cipher text will be exactly same as that of the plain text.<br /><br />The problem occures with the last block. If the length of the plain text is not a multiple of 128. the last block will be shorter than 128 bits. Since the algo can work only on blocks of 128 bits , It is a common practice to pad the last block so that it becomes equal to 128 bits in lenght. This padding is subsequently discarded after decryption.<br /><br />Note: The actual algo is more complicated than this. We have deliberately left out details that are not relevent for this post.<br /><br />This is the encryption method in the PHP integration kit published by the gateway service provider<br /><br /> 1 function encrypt($plainText ,$key)<br /> 2 {<br /> 3   $secretKey = hextobin(md5($key));<br /> 4   $initVector = &ldquo;...&ldquo;<br /> 5   $openMode = mcrypt_module_open(MCRYPT_RIJNDAEL_128 , &amp;#39;&amp;#39; ,&amp;#39;cbc&amp;#39; , &amp;#39;&amp;#39;);<br /> 6   $blockSize = mcrypt_get_block_size(MCRYPT_RIJNDAEL_128 , &amp;#39;cbc&amp;#39;);<br /> 7 <br /> 8   $plainPad = pkcs5_pad($plainText , $blockSize);  //  &#60;;---- Padding<br /> 9 <br />10   if (mcrypt_generic_init($openMode , $secretKey , $initVector) != -1) <br />11   {<br />12     $encryptedText = mcrypt_generic($openMode , $plainPad);<br />13     mcrypt_generic_deinit($openMode);      <br />14   } <br />15   return bin2hex($encryptedText);<br />16 }<br />17 <br />18 // Padding method<br />19 function pkcs5_pad ($plainText , $blockSize)<br />20 {<br />21   // padding logic here<br />22 }<br /><br />And here is the same implemented in Ruby<br /><br />1 def self.encrypt(plain_text , key)<br />2     secret_key     = Digest::MD5.digest(key)<br />3     cipher         = OpenSSL::Cipher::AES.new(128 , :CBC)<br />4     cipher.encrypt<br />5     cipher.key     = secret_key<br />6     cipher.iv      = INIT_VECTOR<br />7     encrypted_text = cipher.update(plain_text) + cipher.final<br />8     return (encrypted_text.unpack(&ldquo;H*&ldquo;)).first<br />9 end<br /><br />Notice any difference?<br /><br />It turns out that , unlike in Python , PHP and few other languages , Ruby wrapper for OpenSSL automatically takes care of padding (default behaviour). This is clearly mentioned in the documentation. For some reason , techies at our gateway service provider overlooked this and hit a dead-end.<br /><br />By the they , they were gracious enough to acknowledge our contribution in their Ruby Integration Kit (accessible only to their subscribers)<br /><br />But We have open sourced our code here ‘cca_crypto’. We have plans of make this into a complete package - with view generators etc. , and publish this as a rubygem. We shall gladly accept any pull request!<br /><br />"
		} ,
	
		{
		  "title"    : "Setting Up Amazon RDS as a Slave to a self-managed MySQL server",
		  "category" : "technology",
		  "url"     : "/technology/setting-up-amazon-rds-as-a-slave-to-a-self-managed-mysql-server/",
		  "date"     : "2014-05-21 00:00:00 +0530",
		  "content"	: "Last week , we migrated our MySQL database server , which was running on an EC2 instance , to RDS. We hoped the migration process would be smooth.<br /><br />As always , migrating a large database has its challenges. Business folks expect the minimum possible downtime.<br /><br />The plan was simple.<br /><br /><br />  Launch an RDS instance<br />  Load a full dump into it<br />  Configure it to act as a slave of the self-managed server (current master)<br />  On the D-day , pull the website down and promote the RDS instance to take over as the new master<br /><br /><br />We soon discovered that RDS comes with curtailed root permissions. There are several commands that are disallowed. Some of these include “CHANGE MASTER TO….”<br /><br />What do we do now?<br /><br />One option was to carry out the migration in one go , while the website was offline. This meant the downtime would have been several hours , instead of minutes. Obviously , not an acceptable option at all.<br /><br />Some R&amp;amp;D was all it took to discover how to proceed with the original approach.<br /><br />RDS comes with a bunch of stored procedures , which help you configure it as a slave. There is almost a one-to-one mapping of these stored procedures with the commands that are disallowed.<br /><br /><br />MySQL CommandCorrosponding Stored Proc<br />CHANGE MASTER TOmysql.rds_set_external_master<br />START SLAVEmysql.rds_start_replication<br />STOP SLAVEmysql.rds_stop_replication<br />RESET MASTERmysql.rds_reset_external_master <br /><br /><br />So , Using these stored procedures , you can now configure your RDS instance as a slave to your self-managed MySQL server<br /><br />After loading a full dump to RDS , Call the stored procedure mysql.rds_set_external_master like this<br /><br />CALL mysql.rds_set_external_master ('servername' , port , 'user' , 'password' , 'binlog-file' , binlog-offset , 0);<br /><br /><br />Then<br /><br />CALL mysql.rds_start_replication;<br /><br /><br />This will make RDS a slave of your self managed mysql server. You can run “SHOW SLAVE STATUS” to see its working.<br /><br />When it is time to promote RDS to master. You call these stored procedures<br /><br />CALL mysql.rds_stop_replication;<br /><br />CALL mysql.rds_reset_external_master;<br /><br /><br />That’s it. Now point your applications to the RDS instance and take your site live.<br /><br />Note:<br /><br />For your RDS to work as a slave , it needs permissions to connect to port 3306 of your current master. Make sure you open this port for the RDS instance.<br /><br />You can run the following command to find out the ip address of your rds instance<br /><br />ping -c rdsname.cpesx66wwe7y.ap-southeast-1.rds.amazonaws.com<br /><br />"
		} ,
	
		{
		  "title"    : "Beware of creating $HOME/.ssh folder by hand, when SELinux is turned on",
		  "category" : "technology",
		  "url"     : "/technology/beware-of-creating-ssh-folder-by-hand-when-selinux-is-turned-on/",
		  "date"     : "2012-07-22 00:00:00 +0530",
		  "content"	: "I was experimenting with chef to manage our Linux boxes. As a standard practice , our application user deployer is homed in /applications/deployer rather than the usual /home/deployer.<br /><br />To enable password less login , I appended my public key to ~/.ssh/authorized_keys<br /><br /> ssh-copy-id -i ~/.ssh/id_rsa deployer@remote.server<br /><br /><br />The first time I run this command , I will be prompted for a password to install my key. After this , I can run the below command to login without a password:<br /><br />ssh -i ~/.ssh/id_rsa deployer@remote.server<br /><br /><br />However , that did not work as expected.<br /><br />For some reason , sshd was unable to read the authorized_keys file. I checked all the usual things.. all looked fine. Everything seem to work just fine when SELinux was running in permissive mode on the remote server , but not when it was in enforcing mode.<br /><br />Discovered that if .ssh folder was created by hand (or even the folder containing .ssh folder) , we need to do few additional things.<br /><br />Step 1:<br /><br />Open this file /etc/selinux/targeted/contexts/files/file_contexts.homedirs and append the following line to the bottom<br /><br /> /applications/deployer/[^/]*/ssh(/.*)?     system_u:object_r:ssh_home_t:s0<br /><br /><br />Note: remember to adjust the path as per your needs.<br /><br />Step 2: run the following command<br /><br />restorecon -R -v /applications/deployer/.ssh<br /><br /><br />Again , remember to adjust the path as per your needs.<br /><br />Now you are all set!<br /><br /> ssh -i ~/.ssh/id_rsa deployer@remote.server<br /><br /><br />should log you in without asking for a password!<br />"
		} ,
	
		{
		  "title"    : "Importance of Date field in an email&#39;s Header",
		  "category" : "technology",
		  "url"     : "/technology/importance-of-date-field-in-an-emails-header/",
		  "date"     : "2012-04-04 00:00:00 +0530",
		  "content"	: "So far , we paid little attention to email delivery issues. We knew delivering to rediffmail is a pain. So we discouraged our users from using rediffmail. Apart from that we had FCrDNS and SPF configured and working fine. We had also configured DKIM. And then a month ago , we also added DMARC in monitor mode.<br /><br />We were happy! Until…<br /><br />Recently , we started getting loads of phishing emails from what appeared to originate from our own domain name [not our servers].<br /><br />It told us two things.<br /><br /><br />  eLitmus.com was growing in popularity<br />  We cannot ignore email delivery issue any longer<br /><br /><br />We ran our email through Spam Assassin checks and were surprised to see that we got a score of 6. Anything above 5 is BAD. It’s a straight spam! But we knew we were not spamming. These were transactional emails triggered by our website on certain events , such as New registration , or Forgot Password.<br /><br />It was almost by accident , we noticed that the timezone in the Date header of the email was appearing as +0580. Indian Standard Time (IST) is 5 hours and 30 minutes ahead of UTC. So this value should have been +0530 , not +0580. Apparently , that is good enough reason for Spam Assassin to treat our mails as spam.<br /><br />Tracing backwards , we discovered a bug in our application code and fixed it. It was a single line fix.<br /><br />With this change , Spam Assassin was happy to give us a score of zero.<br /><br />That is just one part of one header. There are ten others which have to be configured correctly.<br /><br />Here is an article with good insights in to how gmail calculates sender reputation. Its a little dated , but still relevent. Sender reputation in a large webmail service (PDF)<br /><br />By the way , here is a nice and free JSon API to check your email’s reputation.<br /><br />"
		} 
	
]